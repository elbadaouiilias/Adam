Optimiseurs SGD et Adam en PyTorch:

Contenu
Implémentations manuelles de SGD et Adam / Réseau de neurones simple (MLP) / Fonction d’entraînement / Comparaison graphique des pertes (loss) sur 200 epochs

Résultats
Adam converge plus rapidement et de façon plus stable que SGD avec momentum, qui reste cependant efficace.
